# Codex Review – Track Catalogue Update Flow

Scope: how tracks move from LiveRC into the catalogue (cron + manual triggers), how admins follow/unfollow tracks, and how that data feeds downstream features such as weather geocoding. Docker/cron context: scheduled syncs run inside the ingestion container via `ingestion/scripts/run-track-sync.sh`, while the admin UI calls `/api/v1/admin/ingestion` which in turn hits the ingestion service's FastAPI endpoints.

**Cursor Question or Comment:** The scope mentions "manual triggers" but doesn't clarify if this includes both CLI manual runs (`python -m ingestion.cli ingest liverc refresh-tracks`) and HTTP-admin-triggered syncs, or just one. The review focuses on HTTP-admin-triggered syncs, but manual CLI runs are also possible. Should the scope explicitly distinguish between these two manual trigger paths?

## Findings

1. **Two different "track sync" implementations with different behavior**  
   *Evidence: `ingestion/cli/commands.py:366-520`, `ingestion/api/routes.py:74-138`, `src/core/admin/ingestion.ts:20-59`, `docs/api/api-reference.md:1652-1707`.*  
   Cron/manual CLI runs (`refresh-tracks`) do all the heavy lifting: they fetch dashboard metadata per track (lat/long, address, stats), respect the `MRE_SCRAPE_ENABLED` kill-switch, emit markdown reports in `docs/reports`, and prune old reports each run. The admin UI, however, calls the ingestion service's `/tracks/sync` HTTP route, which only fetches the bare catalogue list (`list_tracks`), never touches the dashboard metadata, never checks the kill-switch, and never generates a report. The documentation and UI copy promise "Synchronize the track catalogue from LiveRC," but the HTTP path only toggles `track_name/URL/is_active`. Any admin-triggered sync therefore onboards new tracks without coordinates, addresses, or lifetime stats—exactly the fields the weather service relies on ("Track metadata … is extracted during track sync," docs/api/api-reference.md:1246-1255). Worse, the kill-switch that ops uses to pause scraping (`run-track-sync.sh:11-29`) is silently bypassed, so an admin clicking the button during an outage will still hammer LiveRC. To keep the catalogue consistent (and compliant with ops policy) the admin route either needs to invoke the same CLI workflow or the ingestion HTTP handler must be brought to parity (metadata fetch, kill-switch enforcement, reporting/cleanup).

   **Cursor Question or Comment:** The HTTP route doesn't call `_ensure_scraping_enabled()` or check `SITE_POLICY.ensure_enabled()`, but the CLI route does (`ingestion/cli/commands.py:369`). However, the HTTP route is inside the FastAPI service which could theoretically have access to `SITE_POLICY` if imported. The kill-switch check in `run-track-sync.sh` happens at the shell script level, but the HTTP route bypasses this entirely. Should the HTTP route check `SITE_POLICY.is_enabled()` before proceeding, or should it call `_ensure_scraping_enabled("refresh-tracks")`? The CLI uses `_ensure_scraping_enabled()` which internally calls `SITE_POLICY.ensure_enabled(command)` (`ingestion/cli/commands.py:42-48`), so there's a clear pattern to follow.

   **Cursor Question or Comment:** The CLI implementation has graceful degradation for metadata fetching - if `fetch_track_metadata()` fails for a track, it logs a warning and continues (`ingestion/cli/commands.py:396-403`). The HTTP route doesn't attempt metadata fetching at all, so this isn't an issue, but if we add metadata fetching to the HTTP route, we should ensure the same error handling pattern is used. Should metadata fetch failures be non-fatal in the HTTP route as well?

   **Cursor Question or Comment:** The CLI tracks detailed change information (`changes` list tracking which fields changed - `ingestion/cli/commands.py:413-422`), but the HTTP route's update logic is simpler and doesn't track changes (`ingestion/api/routes.py:107-110` just increments `tracks_updated` without tracking what changed). This detail isn't critical for functionality, but if we want parity, should the HTTP route also track which fields changed for reporting purposes?

   **Cursor Question or Comment:** The repository's `upsert_track()` method has conditional metadata updates - it only updates metadata fields if they're provided (not None), preserving existing data (`ingestion/db/repository.py:150-178`). The CLI passes metadata kwargs only when `dashboard_metadata` is available (`ingestion/cli/commands.py:442-461`), but the HTTP route doesn't pass any metadata kwargs at all (`ingestion/api/routes.py:112-120`). This means the HTTP route won't overwrite existing metadata, but it also won't populate metadata for new tracks. Is this intentional, or should we ensure new tracks get metadata populated on first sync?

   **Cursor Question or Comment:** The CLI uses `asyncio.run()` to call `fetch_track_metadata()` synchronously within the sync loop (`ingestion/cli/commands.py:396`). This is a sequential operation - each track's metadata is fetched one at a time. With 1000+ tracks, this could be slow. Should we consider batching or parallelizing metadata fetches? Or is this sequential approach intentional to avoid hammering LiveRC?

   **Cursor Question or Comment:** What happens if `list_tracks()` returns duplicate `source_track_slug` values? Both implementations use `seen_slugs` to track slugs, but if `list_tracks()` itself returns duplicates, the first occurrence would be processed and the second would be skipped silently. Should we add validation to detect and log duplicate slugs from the API response?

2. **Track sync runs inline over HTTP with no job control**  
   *Evidence: `src/core/admin/ingestion.ts:20-59`, `src/app/api/v1/admin/ingestion/route.ts:23-64`, `ingestion/api/routes.py:74-138`.*  
   When an admin clicks "Trigger Track Sync" the Next.js server makes a blocking `fetch` to `INGESTION_SERVICE_URL/api/v1/tracks/sync`. That FastAPI handler loops over every track before responding; there is no job queue, no background worker, and no throttling. Once the catalogue grows past ~1k entries, that HTTP call routinely runs for tens of seconds, yet the Next.js request still has to stay open (the admin UI just sees a spinner). If the sync takes longer than the platform timeout (60s for Vercel / 30s for many proxies) the admin receives an error even though the ingestion service keeps running, so they retry and double-trigger the job. There's no dedupe or "already running" guard either (the HTTP handler doesn't check for an active sync), so two admins can start the same long-running scrape concurrently. Consider queuing the job (or at least firing-and-forgetting via a small command service) and returning a lightweight job ID/status instead of blocking the Next.js thread on an unbounded external call.

   **Cursor Question or Comment:** The Next.js route handler (`src/app/api/v1/admin/ingestion/route.ts`) doesn't set an explicit timeout on the `fetch()` call to the ingestion service. What is the default timeout for Node.js `fetch()`? If there's no timeout, and the ingestion service hangs, the Next.js request could hang indefinitely. Should we add a timeout to the `fetch()` call in `triggerTrackSync()`? What timeout value would be appropriate given that syncs can take tens of seconds with 1000+ tracks?

   **Cursor Question or Comment:** If the ingestion service is down or unreachable, the `fetch()` call in `triggerTrackSync()` will throw an error (`src/core/admin/ingestion.ts:64-82`). This error is caught and an audit log is created with action `"ingestion.trigger_track_sync.failed"`, which is good. However, the error message returned to the admin is generic. Should we distinguish between different failure modes (network error, service down, timeout, etc.) to give admins better visibility into what went wrong?

   **Cursor Question or Comment:** The HTTP route uses a single database transaction (`db.commit()` at the end - `ingestion/api/routes.py:132`). If the sync fails partway through (e.g., after processing 500 of 1000 tracks), the entire transaction is rolled back. The CLI uses the same pattern (`session.commit()` at the end - `ingestion/cli/commands.py:488`). Is this intentional? If we want partial success (some tracks updated even if others fail), we'd need a different transaction strategy. However, the current approach ensures atomicity - either all tracks are synced or none are. Is this the desired behavior?

   **Cursor Question or Comment:** The HTTP route has minimal error handling - if `list_tracks()` throws an exception, it propagates up and becomes an HTTP 500 (`ingestion/api/routes.py:74-138`). The CLI has similar minimal error handling, but at least logs errors before they propagate. Should the HTTP route have try/except blocks to catch specific exceptions and return more meaningful HTTP status codes (e.g., 503 if LiveRC is unreachable, 400 if request is malformed, etc.)?

   **Cursor Question or Comment:** The HTTP route doesn't log execution metrics like the CLI does. The CLI logs duration, track counts, and report path (`ingestion/cli/commands.py:509-518`), but the HTTP route only logs basic start/success messages (`ingestion/api/routes.py:82, 134-139`). Should the HTTP route log similar metrics (duration_ms, tracks_added, tracks_updated, tracks_deactivated) for observability, even if it doesn't generate a report?

   **Cursor Question or Comment:** The audit log created by `triggerTrackSync()` (`src/core/admin/ingestion.ts:48-58`) records the action and service URL, but doesn't record the sync results (how many tracks were added/updated/deactivated). Should we parse the response from the ingestion service and include sync statistics in the audit log details for better auditability?

   **Cursor Question or Comment:** What happens if two admins trigger syncs simultaneously? Both requests would proceed in parallel, potentially causing database contention or duplicate work. A simple in-memory lock or flag could prevent concurrent syncs, but this wouldn't work across multiple ingestion service instances. For a proper solution, we'd need a distributed lock (Redis, database advisory locks, etc.). Is this a concern given the current single-instance deployment, or should we plan for multi-instance deployments?

3. **Admin "search" only filters the current page**  
   *Evidence: `src/components/admin/TracksTable.tsx:29-188`, `src/app/api/v1/admin/tracks/route.ts:23-44`.*  
   Track management is the only place operators can follow newly synced tracks, but the table fetches just one page (`pageSize` 50) and the "search" box filters that in-memory array. Typing "hobbytech" while you're on page 1 never queries the backend, so you can only find the track if it happens to fall in those 50 rows. With 1k+ tracks you have to page manually until you stumble upon the slug you want to follow. That makes onboarding fresh tracks (the whole point of running track sync) extremely tedious. Either send the search string to `/api/v1/admin/tracks` so the server filters, or request all rows before searching (which won't scale). As-is, the "search" UX gives a false sense of control and turns routine updates into a manual Easter egg hunt.

   **Cursor Question or Comment:** The `getTracks()` function in `src/core/admin/tracks.ts` doesn't accept a search query parameter, so the API route can't filter by search term (`src/app/api/v1/admin/tracks/route.ts:45-60`). To add server-side search, we'd need to: (1) add a `search` parameter to `getTracks()`, (2) construct a Prisma `where` clause that filters by `trackName` (and possibly `sourceTrackSlug`), (3) pass the search param from the API route, and (4) update the UI to send the search query as a URL param. Should the search be case-insensitive? Should it search multiple fields (name, slug, URL)?

   **Cursor Question or Comment:** The client-side filter in `TracksTable.tsx` only filters by `trackName` (`src/components/admin/TracksTable.tsx:109-115`). Should server-side search also only search by name, or should it include other fields like slug or URL? The slug might be more useful for admins who know the LiveRC track slug.

   **Cursor Question or Comment:** When a search query is active, should pagination reset to page 1? Currently, the search is client-side so pagination doesn't apply, but if we move search to the server, we need to decide: should changing the search query reset to page 1, or should we maintain the current page? Typically, search queries reset pagination.

   **Cursor Question or Comment:** The `TracksTable` component uses `useCallback` and `useEffect` to refetch tracks when filters change (`src/components/admin/TracksTable.tsx:43-82`). If we add server-side search, should the search query trigger a refetch on every keystroke (with debouncing), or only on Enter/submit? Debouncing would be better for performance, but might feel less responsive.

4. **No audit/reporting trail for manually triggered syncs**  
   *Evidence: `ingestion/cli/commands.py:494-507`, `docs/operations/liverc-operations-guide.md:150-210`, `docs/reports/track-sync-2025-12-23-13-57-13.md:1-27`, `ingestion/api/routes.py:74-138`.*  
   Ops relies on the markdown reports under `docs/reports/` to review what changed ("Track refresh completed … New/Updated/Deactivated" – operations guide). CLI runs create those reports and prune old ones. The `/tracks/sync` HTTP path creates no report, so the repo stays silent even though the catalogue changed. That makes it impossible to confirm whether an emergency manual sync added or removed anything, and it breaks the documented retention policy (`TRACK_SYNC_REPORT_RETENTION_DAYS`). If manual runs are going to remain HTTP-based, they still need to emit the same report artefacts so ops can audit catalogue changes.

   **Cursor Question or Comment:** The report generation logic (`generate_track_sync_report()` - `ingestion/cli/commands.py:65-175`) writes to `docs/reports/` which is mounted as a volume in Docker (`docker-compose.yml` or ingestion container config). If the HTTP route runs in a different container or context than the CLI, does it have write access to the same volume? The `get_reports_directory()` function checks for `/app/docs/reports` first (Docker path) then falls back to a relative path (`ingestion/cli/commands.py:51-62`). Should the HTTP route use the same logic, or does it need different path resolution?

   **Cursor Question or Comment:** The HTTP route doesn't track detailed change information like the CLI does. To generate a proper report, the HTTP route would need to: (1) track which tracks are new (like CLI does with `new_tracks` list), (2) track which tracks are updated and what changed (like CLI does with `updated_tracks` and `changes` list), and (3) track which tracks are deactivated (like CLI does with `deactivated_tracks` list). The HTTP route currently only counts tracks but doesn't collect the details needed for report generation. Should we refactor to collect this data even if we don't generate reports initially?

   **Cursor Question or Comment:** The `cleanup_old_reports()` function is called by the CLI after report generation (`ingestion/cli/commands.py:507`). If the HTTP route generates reports, should it also call `cleanup_old_reports()`? Or should cleanup remain a CLI-only responsibility? The cleanup logic uses `TRACK_SYNC_REPORT_RETENTION_DAYS` env var (default 30 days - `ingestion/cli/commands.py:182`). If multiple code paths generate reports, we want to ensure cleanup happens consistently.

   **Cursor Question or Comment:** What happens if report generation fails? The CLI calls `generate_track_sync_report()` and logs the path, but if file writing fails (permissions, disk full, etc.), the exception would propagate. The CLI doesn't wrap report generation in try/except, so a report write failure would cause the entire sync to fail. Should report generation be non-fatal (try/except with logging) so that sync can complete even if report writing fails?

   **Cursor Question or Comment:** The audit log created by `triggerTrackSync()` (`src/core/admin/ingestion.ts:48-58`) records that a sync was triggered, but doesn't record the sync results. If we add report generation to the HTTP route, should we also include the report path or sync statistics in the audit log? This would provide a cross-reference between audit logs and report files.

   **Cursor Question or Comment:** The report files are named with timestamps (`track-sync-YYYY-MM-DD-HH-MM-SS.md` - `ingestion/cli/commands.py:97-98`). If an admin triggers multiple syncs in quick succession (e.g., clicks the button multiple times due to slow UI response), we'd get multiple reports with different timestamps. Is this acceptable, or should we add deduplication logic (e.g., if a sync completes within N seconds of the previous one, append to the existing report rather than creating a new one)?

5. **[Codex] HTTP sync stats increment even when nothing changed**  
   *Evidence: `ingestion/api/routes.py:99-118`.*  
   The FastAPI route bumps `tracks_updated` for every catalogue row that already exists, regardless of whether any field actually changed. On today’s 1k+ track list that means the response always reports ~1k “updates,” even if LiveRC returned an identical payload. CLI-led syncs only count rows whose metadata differed, and the markdown report lists what changed. Because manual HTTP runs emit neither change lists nor accurate counts, ops can’t tell whether a given button press actually onboarded anything (and downstream tooling that watches those counters will see constant false positives).

6. **[Codex Question] Why does the admin UI drop the ingestion response?**  
   *Evidence: `src/core/admin/ingestion.ts:23-54`, `src/components/admin/IngestionControls.tsx:92-137`.*  
   `triggerTrackSync` only checks `response.ok` and never reads the JSON summary that `/api/v1/tracks/sync` returns (counts of added/updated/deactivated). The admin control surface consequently just flashes a toast—there’s no confirmation of what happened, and nothing to paste into the LiveOps logbook. Shouldn’t we surface the response payload (or at least stream a short-lived status object) so administrators know whether their manual sync actually touched the catalogue?

7. **[Codex] Manual sync path bypasses the shared kill switch entirely**  
   *Evidence: `src/lib/site-policy.ts:1-33`, `src/lib/ingestion-client.ts:432-488`, `src/core/admin/ingestion.ts:23-67`, `ingestion/api/routes.py:74-138`.*  
   Everywhere else we talk to LiveRC we call `assertScrapingEnabled()` first so ops can flip `MRE_SCRAPE_ENABLED=false` and stem traffic (e.g., event discovery and event ingestion both guard with `assertScrapingEnabled`). The admin-only track sync trigger never calls that helper, so the Next.js server will happily call the ingestion service even when ops has flipped the kill switch in the UI tier. Once the request lands, the FastAPI handler also lacks the `_ensure_scraping_enabled("refresh-tracks")` check the CLI uses, so the kill switch is ignored twice. That undermines the incident response workflow documented in `docs/operations/liverc-operations-guide.md:180-220`, which assumes a single flag can halt scraping. We should gate `triggerTrackSync` with `assertScrapingEnabled` and have the ingestion route call the same policy object (`SITE_POLICY.ensure_enabled`) that the CLI path uses.

8. **[Codex] Admin UX/spec promises job tracking but no job model exists**  
   *Evidence: `docs/user-stories/user-journeys.md:1490-1540`, `src/app/api/v1/admin/ingestion/route.ts:18-79`, `src/core/admin/ingestion.ts:20-118`.*  
   The journey doc spells out an ingestion job list (`GET /api/v1/admin/ingestion/jobs`), confirmation dialog, async job records, etc., but the implemented Next.js route simply forwards the request to FastAPI and immediately returns. There is no `jobs` table, no API endpoints to read job status, no polling, and no UI components referencing a job model. Ops therefore can’t monitor whether the manual sync they just fired is still running, succeeded, or failed—contrary to the documented flow. Either the doc needs to be rescoped or we need to add at least a tiny job record (`triggered_by`, `started_at`, `completed_at`, `status`) so admins aren’t guessing while the long-running HTTP call spins.

9. **[Codex Question] Should ingestion service log who triggered manual syncs?**  
   *Evidence: `src/core/admin/ingestion.ts:23-67`, `ingestion/api/routes.py:74-138`, `docs/operations/liverc-operations-guide.md:198-230`.*  
   Next.js writes an audit entry per button click, but the ingestion service itself has no notion of the triggering admin—the `/tracks/sync` payload is empty and FastAPI only logs `sync_tracks_start`. During postmortems ops usually grep the ingestion logs first (per the ops guide), yet there’s no way to correlate “track sync triggered by Alex” with a specific ingestion log line. Should we pass the admin user ID / request ID in the POST body or headers so FastAPI can annotate its logs (and maybe include it in the markdown report once HTTP achieves parity)?

## Suggested next steps

1. Make the admin-triggered sync call the CLI workflow (or enhance the FastAPI handler to fetch dashboard metadata, respect `MRE_SCRAPE_ENABLED`, and write reports) so manual and scheduled runs behave the same.
2. Push the track-sync trigger into a background job or queue and return a job token/status instead of blocking the Next.js request for the whole scrape. Add basic dedupe ("sync already running") to avoid concurrent double-runs.
3. Wire the admin table's search box into the server query (or provide a dedicated lookup by slug/name) so operators can actually find and follow the tracks that the sync just added.
4. Ensure manual syncs generate the same markdown report artefacts and go through the same retention cleanup logic so ops retains an audit trail regardless of how the job was initiated.

---

## Additional Findings from Deep Review

**Cursor Question or Comment:** The CLI implementation uses `db_session()` context manager for database transactions (`ingestion/cli/commands.py:383`), while the HTTP route uses FastAPI's dependency injection (`db: Session = Depends(get_db)` - `ingestion/api/routes.py:75`). Both ultimately use SQLAlchemy sessions, but the transaction management might differ. The CLI explicitly commits at the end (`ingestion/cli/commands.py:488`), and the HTTP route also commits (`ingestion/api/routes.py:132`). However, FastAPI's dependency injection might handle transaction boundaries differently. Should we verify that both paths have the same transaction semantics (atomicity, rollback on error, etc.)?

**Cursor Question or Comment:** The CLI route uses `asyncio.run()` to call async functions (`connector.list_tracks()`, `connector.fetch_track_metadata()`) within a synchronous context (`ingestion/cli/commands.py:377, 396`). The HTTP route is already async, so it can use `await` directly (`ingestion/api/routes.py:89`). This is fine, but it's worth noting that the CLI's use of `asyncio.run()` creates a new event loop for each call, which is less efficient than reusing an event loop. However, this is a CLI-specific concern and doesn't affect the HTTP route.

**Cursor Question or Comment:** The `LiveRCConnector` is instantiated in both code paths (`ingestion/cli/commands.py:374`, `ingestion/api/routes.py:85`). The connector likely maintains HTTP client state (connections, cookies, etc.). If the connector is instantiated per-request in the HTTP route, this is fine. However, if we want to reuse connections or maintain session state, we might want to use dependency injection or a singleton pattern. Is connection reuse a concern, or is per-request instantiation acceptable?

**Cursor Question or Comment:** The HTTP route returns a simple dictionary with counts (`ingestion/api/routes.py:141-144`), but the CLI command outputs text to stdout via `click.echo()` (`ingestion/cli/commands.py:520-524`). The Next.js route handler receives the JSON response but doesn't seem to use the counts for anything (`src/core/admin/ingestion.ts:60-63` just checks `response.ok`). Should we surface the sync statistics (tracks_added, tracks_updated, tracks_deactivated) in the UI, or is the current "success/failure" feedback sufficient?

**Cursor Question or Comment:** The repository's `upsert_track()` method updates `last_seen_at` timestamp on every update (`ingestion/db/repository.py:147`). This is useful for tracking when a track was last synced. However, if the HTTP route doesn't fetch metadata and only updates basic fields, the `last_seen_at` will be updated even though the track wasn't fully synced. Is this misleading, or is it acceptable to update `last_seen_at` for any sync operation (even partial)?

**Cursor Question or Comment:** The HTTP route's response includes `tracks_added`, `tracks_updated`, and `tracks_deactivated` counts (`ingestion/api/routes.py:141-144`), but the counting logic is simplistic - it increments `tracks_updated` if a track exists, and `tracks_added` if it doesn't, without checking if any fields actually changed (`ingestion/api/routes.py:107-110`). The CLI route tracks actual changes (`ingestion/cli/commands.py:413-422`). This means the HTTP route might report "10 tracks updated" even if none of them actually changed. Should we add change detection to the HTTP route, or is the current behavior acceptable?

**Cursor Question or Comment:** The CLI route uses `click.echo()` for output, which goes to stdout and can be captured by cron logs (`ingestion/cli/commands.py:520-524`). The HTTP route uses structured logging (`logger.info()` - `ingestion/api/routes.py:134-139`). Both are appropriate for their contexts, but the logging formats differ. Should we ensure consistent log message formats between CLI and HTTP routes for easier log aggregation and analysis?

**Cursor Question or Comment:** The HTTP route doesn't have any retry logic for the `list_tracks()` call. If LiveRC is temporarily unavailable, the sync will fail immediately. The CLI route also doesn't have retry logic, so this is consistent, but should we consider adding retries with exponential backoff for transient failures? This would make the sync more resilient to temporary network issues or LiveRC hiccups.

**Cursor Question or Comment:** The HTTP route doesn't validate the response from `list_tracks()`. What if `list_tracks()` returns an empty list? The sync would proceed, mark all existing tracks as inactive, and commit. This might be intentional (if LiveRC returns no tracks, maybe all tracks should be deactivated), but it could also indicate an API error. Should we add validation to ensure we got a reasonable number of tracks (e.g., warn if the count is unexpectedly low)?

**Cursor Question or Comment:** The HTTP route processes tracks sequentially in a loop (`ingestion/api/routes.py:97-120`). With 1000+ tracks, this could be slow. The CLI route also processes tracks sequentially, so this is consistent, but should we consider batching or parallelizing track processing? However, parallel processing might cause database contention or overwhelm LiveRC, so sequential processing is probably safer. Is the current sequential approach acceptable given the performance requirements?

**Cursor Question or Comment:** The HTTP route doesn't set any HTTP response headers (Content-Type, etc.). FastAPI should set Content-Type automatically based on the return type, but explicit headers might be clearer. Also, should we set caching headers (Cache-Control: no-cache) to prevent clients from caching the sync response? The sync results are time-sensitive, so caching would be inappropriate.

**Cursor Question or Comment:** The Next.js route handler doesn't validate the request body structure before calling `triggerTrackSync()` (`src/app/api/v1/admin/ingestion/route.ts:44-47`). It uses `parseRequestBody()` which should validate the structure, but the type checking might not catch all issues. Also, the `triggerTrackSync()` function doesn't accept any parameters beyond auth metadata - it always triggers a full sync. Should we support partial syncs (e.g., sync only a specific track, or sync only metadata) in the future? If so, we'd need to extend the API.

**Cursor Question or Comment:** The audit log created by `triggerTrackSync()` includes `serviceUrl` in the details (`src/core/admin/ingestion.ts:54`), but this is just the base URL, not the full endpoint path. Should we include the full endpoint path (`/api/v1/tracks/sync`) for better traceability? Also, should we include the admin's user ID in the ingestion service's logs, or is the audit log in the Next.js service sufficient?
